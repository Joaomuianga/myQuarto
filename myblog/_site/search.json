[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Take a look",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nChoropleth map with ggplot2 and viridis\n\n\n\n\n\n\nR\n\n\nAPI\n\n\nChoropleth map\n\n\nViridis\n\n\nHospitalization\n\n\nCOVID-19\n\n\n\nA briefing of the Choropleth map with ggplot2 and viridis in R \n\n\n\n\n\nJul 24, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 Forecast: US Hospitalizations data\n\n\n\n\n\n\nR\n\n\nAPI\n\n\nForecast\n\n\nARIMA model\n\n\nTime series\n\n\nHospitalization\n\n\nCOVID-19\n\n\n\nA overview of forecasting methods using US COVID-19 Hospitalization data throught API.\n\n\n\n\n\nJul 3, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Surveillance Report using R Quarto\n\n\n\n\n\n\nR\n\n\nVisualization\n\n\nggplot2\n\n\nSummary Table\n\n\n\nThis is an example of how to create a automated report using R Quarto with fake data.\n\n\n\n\n\nJun 30, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nAPI with R to Extract data from the WHO Global Health Observatory (GHO)\n\n\n\n\n\n\nR\n\n\nAPI\n\n\nSummary Table\n\n\nGHO OData API\n\n\n\nHow to Extract a list of indicators available in World Health Organization - Global Health Observatory (GHO) via API and create a summary table.\n\n\n\n\n\nJun 28, 2024\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gho_with_api/post.html",
    "href": "posts/gho_with_api/post.html",
    "title": "API with R to Extract data from the WHO Global Health Observatory (GHO)",
    "section": "",
    "text": "Context / Purpose\nThe Global Health Observatory (GHO) data repository is WHO’s gateway to health-related statistics. It provides access to over 1000 indicators on priority health topics including mortality and burden of diseases, the Millenium Development Goals and more. They are updated as more recent or revised data become available.\nIn this scenario I will create a function to extract the indicator of Nutrition - Prevalence of wasted children under 5 years of age and summarise a table showing the total of countries by region that have their most recent value from the year of 2001 to 2020.\n\n\nInstall packages\nInstall and run packages, they are the fundamental units of reproducible R code. In this case we will run packages for external connections and reproduction of summarized tables.\n\npacman::p_load(\n  rio,              # Import and export files\n  here,             # File pathways\n  jsonlite,         # Read JSON file in R\n  httr,             # Request object in URL or HTML (GET(), POST())\n  glue,             # Interpret string literals that are small, fast, and dependency-free\n  janitor,          # Data cleaning and tables\n  lubridate,        # Working with dates\n  flextable,        # mMke HTML tables \n  officer,          # hHlper functions for tables\n  tidyverse         # Data management and visualization\n)\n\n\n\nFunction to extract data in the GHO OData API\n\nFirst let’s define the global variables, this involves setting the URL, and indicators interested. The Indicator code is: Prevalence of wasted children under 5 years of age.\nSecond, we will create a function that requests and searches the data on the GHO website for R and converts JSON files into a data frame.\n\nCode source: Thanks Kirstin for useful tutorial.\n\n# Global variable\nurl_base &lt;- \"https://ghoapi.azureedge.net/api/\" \n\n# Declare the type of indicators \nindicator_text &lt;- \"prevalence\"      \nindicator_text_wasted &lt;- \"Wasting prevalence among children under 5 years of age\"\nindicator &lt;- \"NUTRITION_WH_2\"\n\n# The function convert a JSON in URL to tibble\nconvert_JSON_to_tbl &lt;- function(url){\n  data &lt;- GET(url)                                                    # Get a url\n  data_df &lt;- fromJSON(content(data, as = \"text\", encoding = \"utf-8\")) # Convert R object to JSON\n  data_tbl &lt;-  map_if(data_df, is.data.frame, list) %&gt;%               # Takes a predicate function.p \n    as_tibble %&gt;%                                                     # Turn the object such as a data frame\n    unnest(cols = c(value)) %&gt;%                                       # Expands a list-column containing dataframe into rows and columns\n    select(-'@odata.context')                                         \n}\n\n\n###### Exploring the data\n\n# URL will provide you with the list of indicators\nall_indicators &lt;-  convert_JSON_to_tbl(glue(url_base,\"Indicator\")) %&gt;% \n  select(-Language)\n\n####### Create a vector for the  indicator requested\n\n# Get the indicator through code\nget_indicator_code &lt;- all_indicators %&gt;% \n  filter(grepl(indicator, IndicatorCode, ignore.case = TRUE))  # grepl search for matches to argument pattern\n\n# Get the indicator through text \nget_indicator_text &lt;- all_indicators %&gt;% \n  filter(grepl(indicator_text, IndicatorName, ignore.case = TRUE))\n\n# Get the indicator through a word \"Wasting\" \nget_indicator_text_wasted &lt;- all_indicators %&gt;% \n  filter(grepl(indicator_text_wasted, IndicatorName, ignore.case = TRUE))\n\n# Join the tables \nindicators &lt;- get_indicator_code %&gt;% \n  union(get_indicator_text) %&gt;% \n  union(get_indicator_text_wasted)\n\n# Show in a list the names of indicator codes\nindicator_codes &lt;- get_indicator_code %&gt;% \n  pull(IndicatorCode)\n\n# Pull data and Convert to a List, however only extract data of interest\n# Each indicator has its own table in the WHO GHO, but in this case we would like to have one table \"Nutrition_WH\"\nindicator_data &lt;- map(indicator_codes, function(code){       # return a list   \n  response &lt;- GET(\n    glue(url_base, code))                                    # get URL and the 2 indicators codes found\n  content(response, \"text\")\n})\n\n\n# Converts the list to tibble with one column per field\nindicator_data_tbl &lt;- map_dfr(indicator_data, ~ fromJSON(.x, \n                                                         simplifyVector = TRUE)  %&gt;%\n                              as_tibble()  %&gt;%\n                              unnest(cols = everything())) \n\nexport(indicator_data_tbl, here(\"data\", \"data_table_raw.csv\"))\n\n\n\nCheck the data\nIt’s always good to check the table, use the skimr::skim(dataset) function it provides summary statistics about variables in data frames, however let see only 5 rows and all columns.\n\n\nData Cleaning\nTime to clean the data_table. let’s use the functions from the tidyverse family of R packages, we just need to rename interested columns using the rename() function and reorder the columns, keeping the original ones at the end.\n\ndata_table &lt;- indicator_data_tbl %&gt;%\n  # Clean column names\n  clean_names() %&gt;%\n  # Rename columns \n  rename(year = time_dim,\n         numeric_value = numeric_value,\n         high_value = high,\n         low_value = low,\n         country_code = spatial_dim,\n         region_code = parent_location_code,\n         region_name = parent_location,\n         indicator_code = id,\n         indicator_name = indicator_code,\n         source = odata_context) %&gt;% \n  # Reorder the position of interest's columns\n  select(indicator_code, indicator_name, region_code, region_name, year, country_code, low_value, high_value, numeric_value, everything())\n\n\n\nSummary Table\nLet’s create a nice table to show how many countries have their most recent value in the year ranges 2001-2005, 2006-2010, 2011-2015, 2016-2020.\n\ndata_table &lt;- import(here(\"data\", \"nutrition_dataset.csv\"))\n\nborder_style &lt;- officer::fp_border(color=\"black\", width=1)\n\n data_table %&gt;%\n  filter(year &gt; 2000, value&gt;0) %&gt;%\n  mutate(range = case_when(\n    year &lt;2006  ~ \"2001-2005\",\n    year &gt;2005 & year &lt;2011 ~ \"2006-2010\",\n    year &gt;2010 & year &lt;2016 ~ \"2011-2015\",\n    year &gt;2015 & year &lt;2021 ~ \"2016-2020\"\n  )) %&gt;%\n  select(range, region_name, country_code, value) %&gt;%\n   \n  distinct(range, country_code, region_name)%&gt;%\n  tabyl(region_name, range, show_na = FALSE)%&gt;%\n  adorn_totals() %&gt;%\n  adorn_percentages(denominator = \"col\") %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns(position = \"front\") %&gt;%\n  flextable::qflextable() %&gt;% \n  autofit() %&gt;%\n    \n  add_header_row(\n    top = TRUE,\n    values = c(\"WHO Region\",\n               \"Total of Countries by Year Range\",\n               \"\",\n               \"\",\n               \"\")) %&gt;%\n  set_header_labels(\n    region_name = \"\") %&gt;%\n  merge_at(i = 1, j= 2:5, part = \"header\")%&gt;%\n  border_remove() %&gt;%\n  theme_booktabs()%&gt;%\n  vline(part = \"all\", j=1, border = border_style)%&gt;%\n  align(align = \"center\", j = c(2:5), part = \"all\") %&gt;%\n  fontsize(i = 1, size = 12, part = \"header\") %&gt;%\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;%\n  bold(i = 7, bold = TRUE, part = \"body\") %&gt;%\n  bg(part = \"body\", bg = \"gray95\")\n\nWHO RegionTotal of Countries by Year Range2001-20052006-20102011-20152016-2020Africa25  (28.1%)35  (31.5%)41  (36.3%)35  (34.3%)Americas18  (20.2%)22  (19.8%)20  (17.7%)19  (18.6%)Eastern Mediterranean13  (14.6%)12  (10.8%)15  (13.3%)12  (11.8%)Europe18  (20.2%)15  (13.5%)16  (14.2%)14  (13.7%)South-East Asia7   (7.9%)11   (9.9%)8   (7.1%)10   (9.8%)Western Pacific8   (9.0%)16  (14.4%)13  (11.5%)12  (11.8%)Total89 (100.0%)111 (100.0%)113 (100.0%)102 (100.0%)\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/auto_report/auto_report.html",
    "href": "posts/auto_report/auto_report.html",
    "title": "Automated Surveillance Report using R Quarto",
    "section": "",
    "text": "Load Packages\nWe will load janitor for data cleaning, tidyverse for data management and visualization, epikit, apyramid, to import data, cleaning, create plot and table presentation.\n\n# This chunk loads packages\npacman::p_load(\n     rio,          # for importing data\n     here,         # for locating files\n     skimr,        # for reviewing the data\n     janitor,      # for data cleaning  \n     epikit,       # creating age categories\n     gtsummary,    # creating tables  \n     RColorBrewer, # for colour palettes\n     viridis,      # for more colour palettes\n     scales,       # percents in tables  \n     flextable,    # for making pretty tables\n     gghighlight,  # highlighting plot parts  \n     ggExtra,      # special plotting functions\n     matchmaker, # dictionary-based cleaning\n     tsibble,      # for epiweek \n     forcats,      # Missing values\n     scales,       # works with date\n     apyramid,     # age pyramid\n     rstatix,      # summary statistic\n     ggExtra,      # special plotting functions\n     tidyverse     # for data management and visualization\n     \n)\n\n\n\nImport dataset\nThis is a fake data of fever outbreak in Mali 2020.\n\n# This chunck import the linelist of yellow fever\nyellow_fever_raw &lt;- import(here(\"data\", \"yelow_fever_ mali_09_01_2020.xlsx\"))\n\n\n\nCode\nskimr::skim(yellow_fever_raw)\n\n\n\n\nData Cleaning\n\nyellow_fever_clean &lt;- yellow_fever_raw%&gt;%\n# automatically clean column name\n  janitor::clean_names() %&gt;%\n  \n# manually clean column names to English\n  rename(\n    #new            #old\n    n_order =num_ordre,\n    age_month = age_mois,\n    age_year = age_an,\n    gender = sexe,\n    health_area = aire_de_sante,\n    district_not = district_sanitaire_notifiant,\n    district_res = district_sanitaire_de_residence,\n    date_onset = date_de_consultation,\n    fever = fievre,\n    symptoms = symptomes,\n    date_death = date_de_deces,\n    classification = classification_finale,\n    outcome = evolution\n  ) %&gt;%\n\n# de-duplicate rows\n  distinct()%&gt;%\n  \n#convert date onset to date class\n  mutate(date_onset = ymd(date_onset)) %&gt;%\n  \n  # convert age (month and year) to numeric class\n  mutate(age_month = as.numeric(age_month),\n         age_year = as.numeric(age_year)) %&gt;%\n  \n  # convert \"Unknown\" gender to NA\n  mutate(gender = na_if(gender, \"Unknown\"))%&gt;%\n  \n  # create column epiweek \n  mutate(epiweek = floor_date(date_onset, \n                    unit = \"week\",\n                    week_start = 1\n                              )) %&gt;%\n  \n# properly record missing values in many character columns\n  mutate(across(.cols = where(is.character), .fns = ~na_if(.x, \"\"))) %&gt;%\n  \n# re-code status columns\n  mutate(status = recode(status,\n          # for reference: OLD  = NEW                 \n          \"Confirmé\" = \"confirmed\",\n          \"Probable\" = \"probable\",\n          \"Suspect\" = \"suspected\" ))%&gt;%\n  \n# re-code outcome columns\n  mutate(outcome= recode(outcome,\n          # for reference: OLD  = NEW                 \n          \"Vivant\" = \"alive\",\n          \"Décédé\" = \"dead\"))%&gt;%\n  \n      # re-code classification columns\n  mutate(classification = recode(classification,\n          # for reference: OLD  = NEW                 \n          \"Confirme\" = \"confirmed\",\n          \"Probable\" = \"probable\",\n          \"Non cas\" = \"no_case\",\n          \"Suspect\" = \"suspected\"))%&gt;%\n  \n  # re-code some names in symptoms columns\n  mutate(symptoms = recode(symptoms,\n                           # for reference: OLD  = NEW   \n                           \"Fièvre, ictère,\" = \"Fièvre, ictère\",\n                           \"Fievre plus ictere\" = \"Fièvre, ictère\",\n                           \"Icteère, fièvre, vomissement\" = \"Ictère, fivre, Vomissement\",\n                           \"fièvre, ictère, vomissement\" = \"Ictère, fivre, Vomissement\",\n                           \"Fièvre,Fièvre, Vomissemnt\" = \"Fièvre, Vomissemnt\")) %&gt;%\n\n# create column age_years\n  mutate(age_years = case_when(\n    age_year != is.na(age_year) ~ age_year,            # if column is age_year keep age_year\n    age_month !=  is.na(age_month) ~ age_month/12,     # if column age_month is month divide by 12 \n     .default = NULL                                   # if age_year missing assume age_years, else NA\n  )) %&gt;%\n  \n  # transform to numeric\n  mutate(age_years = as.numeric(age_years)) %&gt;%\n\n# age_class column\nmutate(age_class = ifelse(age_years &gt;=18, \"adult\", \"child\")) %&gt;% \n  \n# create age categories column\n  mutate (age_cat = cut(\n    age_years,\n    breaks = c( 0 , 5,  15, 25, 100),          \n    right = FALSE,\n    include.lowest = TRUE,\n    labels = c(\"&lt;5\", \"5-14\", \"15-24\", \"+25\")),\n    \n    age_cat = fct_explicit_na(\n      age_cat,\n     na_level = \"Missing age\"\n    )) %&gt;%\n  \n\n# check similar column in district_res and district_dect\n mutate(moved = district_res != district_not)%&gt;%\n  \n# create new column named district to fill-in missing values\nmutate(district = coalesce(district_not, district_res)) %&gt;%\n  \n# re-code districts columns\n  mutate(district = recode(district,\n          # for reference: OLD  = NEW                 \n          \"Ouelessebougou\" = \"Ouélessebougou\",\n          \"Sefeto\" = \"Sofeto\"))%&gt;%\n  \n# select column in study\n  select(n_order, starts_with(\"district\"), date_onset, epiweek, age_cat, age_years, gender, status, classification, outcome, everything())\n\n\n\nHighlights for Introduction\n\n48 Number of new cases reported during the epi-week 2019-12-30 - N°4 including 5 confirmed cases and 7 deaths.\n48 Cumulative number of cases since the beginning of the outbreaks including 5 confirmed cases, 3 probable cases and 40 suspected cases.\n7 Cumulative number of deaths, providing a case fatality rate of 15%.\n\nThis is the code inline that create the Highlights.\n\n#-   `r nrow(yellow_fever_clean)` Number of new cases reported during the epi-week [ **`r max(yellow_fever_clean$epiweek)` - N°4**]{style=\"color: green;\"} including `r sum(yellow_fever_clean$status == \"confirmed\", na.rm = T)` confirmed cases and `r sum(yellow_fever_clean$outcome == \"dead\", na.rm = T)` deaths.\n#-   `r nrow(yellow_fever_clean)` *Cumulative number of cases since the beginning of the outbreaks including* `r sum(yellow_fever_clean$status == \"confirmed\", na.rm = T)` confirmed cases, `r sum(yellow_fever_clean$status == \"probable\", na.rm = T)` probable cases and `r sum(yellow_fever_clean$status == \"suspected\", na.rm = T)` suspected cases.\n#-   `r sum(yellow_fever_clean$outcome == \"dead\", na.rm = T)` *Cumulative number of deaths*, providing a case fatality rate of `r  round(sum(yellow_fever_clean$outcome == \"dead\", na.rm = T)/nrow(yellow_fever_clean)*100, digits=0)`%.\n\n\n\nSummary Table\n\nborder_style = officer::fp_border(color=\"black\", width=1)\n\n# Table summarizing the number of cases (confirmed, probable, and suspected) by District sanitaire: Cumulative and last epi-week\nyellow_fever_clean %&gt;% \n  group_by(district) %&gt;%\n  summarise(\n            cum_cases = n(),\n            case_1wk = sum(cum_cases[epiweek &gt; max(epiweek)-7], na.rm = TRUE),\n            suspected = sum(status == \"suspected\", na.rm = T),\n            confirmed = sum(status == \"confirmed\", na.rm = T),\n            probable = sum(status == \"probable\", na.rm = T)) %&gt;%\n  filter(cum_cases &gt;1) %&gt;%\n  adorn_totals(where = \"col\") %&gt;%\n  adorn_percentages(\"col\") %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns(position = \"front\") %&gt;%\n  qflextable()  %&gt;%\n  set_header_labels(\n    district = \"District\",\n    cum_cases = \"Cumulative Cases\",\n    case_1wk = \"Cases - Last Week\",\n    suspected = \"Supected cases \",\n    confirmed = \"Confirmed cases \",\n    probable = \"Probable cases \"\n  ) %&gt;%\n   vline(part = \"all\", j = c(1:5), border = border_style)%&gt;%\n   bg(part = \"body\", bg = \"gray95\") %&gt;%\n   bg(., i= ~ district == \"Bougouni\", part = \"body\", bg = \"#91c293\")\n\nDistrictCumulative CasesCases - Last WeekSupected cases Confirmed cases Probable cases TotalBougouni21 (55.3%)0  (0.0%)15 (46.9%)3 (100.0%)3 (100.0%)42 (48.8%)Commune V3  (7.9%)3 (30.0%)3  (9.4%)0   (0.0%)0   (0.0%)9 (10.5%)Kalaban coro2  (5.3%)0  (0.0%)2  (6.2%)0   (0.0%)0   (0.0%)4  (4.7%)Ouélessebougou5 (13.2%)0  (0.0%)5 (15.6%)0   (0.0%)0   (0.0%)10 (11.6%)Sofeto3  (7.9%)3 (30.0%)3  (9.4%)0   (0.0%)0   (0.0%)9 (10.5%)Tombouctou4 (10.5%)4 (40.0%)4 (12.5%)0   (0.0%)0   (0.0%)12 (14.0%)\n\n\n\n\nCases by classification by outcome\nPlot the Distribution of cases by classification and by outcome using geom_bar() function.\n\nggplot(data = yellow_fever_clean, \n       mapping = aes(x = classification,\n                     fill=outcome)) +\n  geom_bar( col = \"black\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"alive\" = \"#1c9099\",\n                              \"dead\" = \"#c51b8a\"),\n                    labels = c(\"alive\" = \"Alive\",\n                                \"dead\" = \"Dead\") )+\n  labs(\n    x = \"Classification\",\n    y = \"Number\",\n    fill = \"Outcome\"\n  ) +\n  theme(\n    axis.title = element_text(size = 14),\n    axis.text.x = element_text(size = 11),\n    axis.text.y = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\nEpicurve of symptoms and classification\nPlot a epicurve showing distribution of cases by week of symptoms onset and classification.\n\nggplot(data = yellow_fever_clean, \n       mapping = aes(x = epiweek)) +\n geom_histogram(aes(fill = classification)) +\n  scale_x_date(date_breaks = \"1 month\",\n               date_labels =  \"%d %b \\n %Y\", \n               expand = c(0,0)) +\n  scale_fill_manual(\n                     values = c( \"confirmed\" = \"#2b8cbe\", \n                                  \"probable\" = \"purple\",\n                                 \"suspected\" = \"#d95f0e\"),\n                    labels = c(\"confirmed\" = \"Confirmed\",\n                               \"probable\"= \"Probable\",\n                               \"suspected\" = \"Suspected\"),\n                    name = \"Classification\") +\n  facet_wrap(~district) +\n  theme_bw() +\n  labs( x = \"Epi- week\",\n        y = \"Cases\",\n        fill = \"Classification\",\n        caption = str_glue(\"A total of {n_distinct(yellow_fever_clean$district)} districts reported  Yellow fever cases \\nCumulative cases of {nrow(yellow_fever_clean)} as of {max(yellow_fever_clean$epiweek)}\"))\n\n\n\n\n\n\n\n\n\n\nAge Pyramid\nPlot the distribution of cases by age group and gender using apyramid function.\n\napyramid::age_pyramid(data = yellow_fever_clean,\n                      age_group = age_cat,\n                      split_by = gender,\n                      show_midpoint = FALSE) +\n  scale_fill_manual(values = c(\"M\" = \"orange\",\n                               \"F\" = \"purple\"),\n                    labels = c(\"M\" = \"Male\",\n                               \"F\" = \"Female\"),\n                    na.value = \"grey\")+\n  labs(title = \"Age Pyramid\",\n       y = \"Cases\",\n       x = \"Age Category\",\n       fill = \"Gender\",\n       caption = str_glue(\"Total cases (%):  in Male {fmt_count(yellow_fever_clean, gender == 'M')} and Female {fmt_count(yellow_fever_clean, gender == 'F')}\")) +\n  theme(\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(size = 14)\n    \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/maps_viridis/maps.html",
    "href": "posts/maps_viridis/maps.html",
    "title": "Choropleth map with ggplot2 and viridis",
    "section": "",
    "text": "How to build\nThis post shows the simple way to load geospatial data, merge with different data formats (csv, excel, etc) and build a Choropleth map with ggplot2, viridis in R\n\n\nAbout Choroplet map\nChoropleth map is composed of colored polygons. It is used to represent spatial variations of a quantity. Making choropleth maps require two main types of inputs 1- Geometry information: - This can either be a supplied GeoJSON file where each feature has either an id field or some identifying value in properties; or - one of the built-in geometries within plot_ly 2- A list of values indexed by feature identifier.\n\n\nImport shapefile\nI will load essential libraries to import data and geospatial analysis.\n\npacman::p_load(rio,             # import function\n               here,            # file path\n               scales,          # percent function\n               janitor,         # summary table \n               sf,              # to manager spatial data using a simple feature format\n               tidyverse        # data management and visualization\n               )\n\nI downloaded the US states shapefile at census.gov website, it’s for public use. Use read_sf() function to import the shapefile in your local computer; use here() function to say what is the file path.\n\n# Import shapefile\nus_states_sf &lt;- read_sf(here(\"gis\", \"spf_usa\", \"usa_shapefile.shp\")) %&gt;%\n  janitor::clean_names() \n\n\n\nCode\n#|code-fold: false\n\nhead(us_states_sf, 5)\n\n\nSimple feature collection with 5 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -103.0026 ymin: 30.17394 xmax: -75.24227 ymax: 40.6388\nGeodetic CRS:  NAD83\n# A tibble: 5 × 10\n  statefp statens  affgeoid    geoid stusps name           lsad    aland  awater\n  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 28      01779790 0400000US28 28    MS     Mississippi    00    1.22e11 3.93e 9\n2 37      01027616 0400000US37 37    NC     North Carolina 00    1.26e11 1.35e10\n3 40      01102857 0400000US40 40    OK     Oklahoma       00    1.78e11 3.37e 9\n4 51      01779803 0400000US51 51    VA     Virginia       00    1.02e11 8.53e 9\n5 54      01779805 0400000US54 54    WV     West Virginia  00    6.23e10 4.89e 8\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\nBasic map\nNow we have a geospatial object called us_stat_sf, we can plot a basic map using ggplot() and geom_df()\n\nggplot(data = us_states_sf)+\n  geom_sf(fill = \"white\", show.legend = FALSE)\n\n\n\n\nbasic_map\n\n\n\n\n\n\nRead the dataset\nThe database is available for download at the healthData.gov, To have the New hospitalization a calculation among some core indicators that match with inpatients admission definition cases and were group by states.\n\n# print first 5 rows of the dataset containing hospitalization\nhosp_data &lt;- import(here(\"data\", \"usa_hosp.csv\"))\n\nhead(hosp_data, 5)\n\n  state new_hospitalization\n1    AK               10651\n2    AL              134959\n3    AR               77594\n4    AS                 225\n5    AZ              173993\n\n\n\n\nMerge geospatial and numeric data\nThe id in common is shapefile: stusps and dataset state\n\n# Join or merge geospatial and numeric data\nus_states_sf &lt;- us_states_sf %&gt;%\n  left_join(hosp_data, by = c(\"stusps\" = \"state\"))\n\nhead(us_states_sf, 5)\n\nSimple feature collection with 5 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -103.0026 ymin: 30.17394 xmax: -75.24227 ymax: 40.6388\nGeodetic CRS:  NAD83\n# A tibble: 5 × 11\n  statefp statens  affgeoid    geoid stusps name           lsad    aland  awater\n  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 28      01779790 0400000US28 28    MS     Mississippi    00    1.22e11 3.93e 9\n2 37      01027616 0400000US37 37    NC     North Carolina 00    1.26e11 1.35e10\n3 40      01102857 0400000US40 40    OK     Oklahoma       00    1.78e11 3.37e 9\n4 51      01779803 0400000US51 51    VA     Virginia       00    1.02e11 8.53e 9\n5 54      01779805 0400000US54 54    WV     West Virginia  00    6.23e10 4.89e 8\n# ℹ 2 more variables: geometry &lt;MULTIPOLYGON [°]&gt;, new_hospitalization &lt;int&gt;\n\n\n\n\nBasic map with numeric variable\nWe will plot a basic choropleth map to show inpatients admission distribution by US states\n\nggplot(us_states_sf) +\n  geom_sf(aes(fill = new_hospitalization))+\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nCustomized choropleth map\nThis step will transform and give a good image to the plot, adding color palette, legend, tittle, scale, etc.\n\nggplot(us_states_sf, mapping = aes(fill = new_hospitalization))+\n  geom_sf(linewidth = 0,\n          alpha = 0.9,\n          position = \"identity\")+\n  geom_sf_text(data = us_states_sf, aes(label = stusps), size = 2.5, colour = \"white\", fontface = \"bold\")+\n  theme_void()+\n  scale_fill_viridis_c(\n    name = \"Number of inpatients admission\",\n    labels = scales::comma,\n    breaks = c(1, 50000, 100000, 200000, 400000, 600000, 800000),\n    guide = guide_legend(\n      keyheight = unit(3, units = \"mm\"),\n      keywidth = unit(12, units = \"mm\"),\n      label.position = \"bottom\",\n      title.position = \"top\",\n      nrow = 1\n    )\n  )+ \n  labs(title = \"COVID-19: Hospitalization in USA\",\n       subtitle = \"Total of inpatients admission from 2020-01-01 to 2024-04-27 in 50 states\")+\n  theme(\n    text = element_text(color = \"#22211d\"),\n    plot.background = element_rect(fill = \"#f5f5f2\", color = NA),\n    panel.background  = element_rect(fill = \"#f5f5f2\", color = NA),\n    legend.background  = element_rect(fill = \"#f5f5f2\", color = NA),\n    \n    plot.title = element_text(\n      size = 18, hjust = 0.01, color = \"#4e4d47\",\n      margin = margin( b= 0.1, t= 0.4, l = 2, unit = \"cm\"\n      )\n    ),\n\n        plot.subtitle = element_text(\n      size = 12, hjust = 0.01,\n      color = \"#4e4d47\",\n      margin = margin(\n        b = -0.1, t = 0.43, l = 2,\n        unit = \"cm\"\n      )\n    ),\n    plot.caption = element_text(\n      size = 10,\n      color = \"#4e4d47\",\n      margin = margin(\n        b = -0.1, r = -99, t = 0.3,\n        unit = \"cm\"\n      )\n    ),\n\n      legend.position = c(0.2, 0.08)\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/forecast_hosp/forecast.html",
    "href": "posts/forecast_hosp/forecast.html",
    "title": "COVID-19 Forecast: US Hospitalizations data",
    "section": "",
    "text": "APIs allow programmers to request data directly from Server API or website through an Application Programming Interface. The first step is making a request to access the data in a certain website, the API server (own of data) will send back a response. There are several types of requests that one can make to an API server GET, POST, PUT. Four our purpose we’ll be asking for data using GET requests.\nTo create a GET request we need to use the GET() function from the httr library.\n\npacman::p_load(httr,            # request url using GET() function\n               jsonlite,        # convert JSON file to a data frame in R\n               forecast,        # fit sin and cosin terms to data\n               trending,        # fit and assess models\n               feasts,          # for time series decomposition and autocorrelation\n               janitor,         # summary table \n               tidyverse        # data management and visualization\n               )\n\nLet’s create an object called path that will be the JSON link. For particularly large datasets, you probably will not be able to grab all rows with a single API call. An alternative is to request the data in batches by using the$limitand $offset parameters. Users can request data beginning at a row other than the first row by specifying the ?$offset=_\" parameter at the end of the URL. For example, for the COVID-19 Reported Patient Impact and Hospital Capacity by State Time Series dataset, \"https://healthdata.gov/resource/g62h-syeh.json?$limit=50000\" will bring in the first 50000 rows\", \"https://healthdata.gov/resource/g62h-syeh.json?$limit=50000&\\$offset=50000\" will bring in the next 50000 rows.\n\n# url for first 50000 rows\npath &lt;- \"https://healthdata.gov/resource/g62h-syeh.json?$limit=50000\" \n\n# url for first 50000 rows\npath_2nd &lt;- \"https://healthdata.gov/resource/g62h-syeh.json?$limit=50000&$offset=50000\"\n\nNow let’s make the request using GET() function\n\nrequest &lt;- GET(url = path) \nrequest_2nd &lt;- GET(url = path_2nd)\n\n\n\n\n# see the URL content\nrequest\n\nResponse [https://healthdata.gov/resource/g62h-syeh.json?$limit=50000]\n  Date: 2024-07-26 11:06\n  Status: 200\n  Content-Type: application/json;charset=utf-8\n  Size: 332 MB\n[{\"state\":\"MA\",\"date\":\"2021-06-15T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"WY\",\"date\":\"2021-06-13T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"LA\",\"date\":\"2021-06-06T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"RI\",\"date\":\"2021-06-04T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"KS\",\"date\":\"2021-06-03T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"ME\",\"date\":\"2021-06-03T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"RI\",\"date\":\"2021-06-02T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"ME\",\"date\":\"2021-05-30T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"RI\",\"date\":\"2021-05-30T00:00:00.000\",\"critical_staffing_shortage_t...\n,{\"state\":\"MS\",\"date\":\"2021-05-28T00:00:00.000\",\"critical_staffing_shortage_t...\n...\n\n\nThe output shows: Date: 2024-07-02 22:49 when the request was made Status: 200 refers to the success or failure of the API request, if the number is 200 the request was successful. Content-Type: application/json the response says that the data takes on json format Size: 332 MB the size of the json file\n\n\n\nJSON is formatted as a series of key-value pairs, where particular word (“key”) is associated a particular value. to do this first let install jsonlite package that contain fromJSON() function that convert JSON file into list. The fromJSON() needs a character vector that contain the JSON structure.\n\n# Convert the raw Unicode into a character vector that resembles the JSON format\nconvert_JSON_to_df &lt;- base::rawToChar(request$content)\n# Get the data we want in a format that we can more easily manipulate in R\nhospitalization_covid_usa &lt;- fromJSON(convert_JSON_to_df, flatten = TRUE)\n\n######### convert the second file\n# Convert the raw Unicode into a character vector that resembles the JSON forma\nconvert_JSON_to_df_2nd &lt;- base::rawToChar(request_2nd$content)\n# Get the data we want in a format that we can more easily manipulate in R\nhospitalization_covid_usa_2nd &lt;- fromJSON(convert_JSON_to_df_2nd, flatten = TRUE)\n\nLet’s join the 2 tables using the rbind() function and clean the data. First I will clean the names of columns using the clean_names() function that standardizes the syntax of column names, and select interested variables. Use the skim() function for the summary table. I will select only 4 variables, state, date, previous_day_admission_adult_covid_confirmed and previous_day_admission_pediatric_covid_confirmed, the indicators are aligned to WHO impatient care definition case, then will sum give Hospitalization cases.\n\n# join data frame using rbind\nhosp_clean &lt;-rbind(hospitalization_covid_usa, hospitalization_covid_usa_2nd) %&gt;%\n  clean_names() %&gt;%\n  replace(is.na(.), 0) %&gt;%\n  # calculate new hospitalization and deaths column\n  mutate(previous_day_admission_adult_covid_confirmed     = as.numeric(previous_day_admission_adult_covid_confirmed),\n         previous_day_admission_pediatric_covid_confirmed = as.numeric(previous_day_admission_pediatric_covid_confirmed),\n         new_hospitalization = previous_day_admission_adult_covid_confirmed + previous_day_admission_pediatric_covid_confirmed,\n         deaths_covid = as.numeric(deaths_covid),\n         date                = as.Date(date)) %&gt;%\n  \n  # Select state, date and new_hospitalization variable\n  select(state, date, new_hospitalization, deaths_covid) %&gt;%\n  \n  # create new variable \n  mutate(country  = \"UNITED STATES OF AMERICA\",\n          ISO_3_CODE = \"USA\")\n\n\ntm_clean &lt;- hosp_clean%&gt;%\n  mutate(yearmonth = zoo::as.yearmon(date)) %&gt;%\n  group_by(yearmonth) %&gt;%\n  summarise(new_hospitalization = sum(new_hospitalization, na.rm = TRUE),\n            deaths_covid = sum(deaths_covid, na.rm = TRUE))"
  },
  {
    "objectID": "posts/forecast_hosp/forecast.html#polar-seasonal-plot",
    "href": "posts/forecast_hosp/forecast.html#polar-seasonal-plot",
    "title": "COVID-19 Forecast: US Hospitalizations data",
    "section": "Polar Seasonal plot",
    "text": "Polar Seasonal plot\nIs similar to a time plot except that the data are plotted against the individual “seasons” in which data were observed. will create using ggseasonplot() function\n# polar seasonal plot for hospitalization\nggseasonplot(tm[,1], polar = TRUE) +\n  ylab(\"New Hospilatization\")+\n  ggtitle(\"Polar seasonal plot: New Hospitalization\")\n# polar seasonal plot for deaths\nggseasonplot(tm[,2], polar = TRUE) +\n  ylab(\"New Hospilatization\")+\n  ggtitle(\"Polar seasonal plot: Deaths\")\n\n\n\n\n\n\n-“hospitalization” -“deaths”\n\n\n\n\n\n\n\n-“hospitalization” -“deaths”"
  },
  {
    "objectID": "posts/forecast_hosp/forecast.html#autocorrelation-of-non-seasonal-time-series",
    "href": "posts/forecast_hosp/forecast.html#autocorrelation-of-non-seasonal-time-series",
    "title": "COVID-19 Forecast: US Hospitalizations data",
    "section": "Autocorrelation of non-seasonal time series",
    "text": "Autocorrelation of non-seasonal time series\nAnother way to look at time series data is to plot each observation against another observation that occurred some time previously by using gglagplot(). For example, you could plot Yt against Yt−1. This is called a lag plot because you are plotting the time series against lags of itself.\nThe correlations associated with the lag plots form what is called the autocorrelation function (ACF). The ggAcf() function produces ACF plots.\n\n# ACF plot of hospitalization data\nggAcf(tm[,2])\n\n\n\n\n\n\n\n\n###White noise\n\niid, or random, independent, individually distributed observations\n\n###Ljung-Box Test\nThe Llung-Box test considers the first h autocorrelation values together\nA significant test (small p-value) indicates the data are probably not white noise.\n###White noise summary\n\nwhite noise is a time series that is purely random\nwe can test for white noise by looking at an ACF plot or by doing a Ljung-Box test\n\n\nBox.test(tm[,1], lag = 24, fitdf = 0, type = \"Ljung\")\n\n\n    Box-Ljung test\n\ndata:  tm[, 1]\nX-squared = 77.144, df = 24, p-value = 1.723e-07"
  },
  {
    "objectID": "posts/forecast_hosp/forecast.html#checking-time-series-residuals",
    "href": "posts/forecast_hosp/forecast.html#checking-time-series-residuals",
    "title": "COVID-19 Forecast: US Hospitalizations data",
    "section": "Checking time series residuals",
    "text": "Checking time series residuals\nWhen applying a forecasting method, it is important to always check that the residual are well-behaved and resemble white noise.\n\ntm[,1] %&gt;% naive() %&gt;% checkresiduals()\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 22.085, df = 10, p-value = 0.01468\n\nModel df: 0.   Total lags used: 10"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "João Muianga",
    "section": "",
    "text": "I’m a Data Analyst and Management with passion in R, Power BI, Excel, SQL, ChatGPT and MLLs, most of my work and experience acquired at UN where I started my career as data manager.\nI’m concentrate my effort and research in Health sector, Food and Nutrition and Demography area, where daily I do data collection, cleaning, manipulation, visualization, statistics analysis and MLL.\nWhen I’m not working, I like to spend my time playing paddle tennis, going on adventures, seeing new places, playing music and spending time with my family and friends.\n\n\nUniversity of Eduardo Mondlane, Maputo | Mozambique.\nComputer Engineering Degree | Feb 2011 - Dec 2016\n\n\n\nI have over 6 years working for UN as Data management and analyst for various humanitarian and emergency response, outbreaks including Global COVID-19.\nI also have computer’s skills such MS Excel, Azure, Cisco Network, Github/Git, ChatGPT and more tools."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "João Muianga",
    "section": "",
    "text": "University of Eduardo Mondlane, Maputo | Mozambique.\nComputer Engineering Degree | Feb 2011 - Dec 2016"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "João Muianga",
    "section": "",
    "text": "I have over 6 years working for UN as Data management and analyst for various humanitarian and emergency response, outbreaks including Global COVID-19.\nI also have computer’s skills such MS Excel, Azure, Cisco Network, Github/Git, ChatGPT and more tools."
  },
  {
    "objectID": "gis/spf_usa/usa_shapefile.html",
    "href": "gis/spf_usa/usa_shapefile.html",
    "title": "João Muianga",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“NAD83”,DATUM[“North American Datum 1983”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Geodesy.”],AREA[“North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.”],BBOX[14.92,167.65,86.46,-47.74]],ID[“EPSG”,4269]] +proj=longlat +datum=NAD83 +no_defs 3401 4269 EPSG:4269 NAD83 longlat EPSG:7019 true"
  }
]